{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6827fc42",
   "metadata": {},
   "source": [
    "## Title : Cluster Analysis on Indian companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a277e1",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0f72b",
   "metadata": {},
   "source": [
    "#### 1. Web Scraping:\n",
    "\n",
    "The data has been scraped from a website called \"ambition box\". The website has information about all the companies present in India, and we tried scraping majority of them. The link to the website is:\n",
    "url = 'https://www.ambitionbox.com/list-of-companies?campaign=desktop_nav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8628913",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''final = pd.DataFrame()\n",
    "for j in range(1, 501):\n",
    "    # The link of company which is being scraped\n",
    "    url = 'https://www.ambitionbox.com/list-of-companies?page={}' .format(j)\n",
    "    # user agent \n",
    "    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'}\n",
    "    webpage=requests.get(url, headers = headers).text\n",
    "    soup=BeautifulSoup(webpage,'lxml')\n",
    "    company = soup.find_all('div', class_ = 'companyCardWrapper')\n",
    "    \n",
    "    name = []\n",
    "    ratings = []\n",
    "    combined = []\n",
    "    company_type = []\n",
    "    employee_count = []\n",
    "    ownership_status = []\n",
    "    company_age = []\n",
    "    headquarters = []\n",
    "    reviews = []\n",
    "    salaries = []\n",
    "    interviews=[]\n",
    "    jobs = []\n",
    "    benefits = []\n",
    "\n",
    "\n",
    "    for i in company:\n",
    "            # Scraping the name of the company\n",
    "            name.append(i.find('h2').text.strip())\n",
    "\n",
    "            # Scraping the ratings of the company\n",
    "            ratings.append(i.find('span', class_='companyCardWrapper__companyRatingValue').text.strip())\n",
    "\n",
    "            # Scraping Reviews, salaries, Interviews, Jobs and Benefits\n",
    "\n",
    "            # Find the div element containing the anchor tags\n",
    "            div_element = i.find('div', class_='companyCardWrapper__tertiaryInformation')\n",
    "\n",
    "            # Check if the div element exists\n",
    "            if div_element:\n",
    "                # Find all anchor tags within the div\n",
    "                anchor_tags = div_element.find_all('a', class_='companyCardWrapper__ActionWrapper')\n",
    "\n",
    "                for anchor_tag in anchor_tags:\n",
    "                    # Find the span tag within each anchor tag with class companyCardWrapper__ActionCount\n",
    "                    span_tag = anchor_tag.find('span', class_='companyCardWrapper__ActionCount')\n",
    "\n",
    "                    if span_tag:\n",
    "                        # Extract and append the text content to the appropriate list\n",
    "                        text_content = span_tag.text\n",
    "                        if \"Salaries\" in anchor_tag.text:\n",
    "                            salaries.append(text_content)\n",
    "                        elif \"Reviews\" in anchor_tag.text:\n",
    "                            reviews.append(text_content)\n",
    "                        elif \"Jobs\" in anchor_tag.text:\n",
    "                            jobs.append(text_content)\n",
    "                        elif \"Interviews\" in anchor_tag.text:\n",
    "                            interviews.append(text_content)\n",
    "                        elif \"Benefits\" in anchor_tag.text:\n",
    "                            benefits.append(text_content)\n",
    "\n",
    "                    else:\n",
    "\n",
    "                         print(\"Span tag not found within the anchor tag.\")\n",
    "\n",
    "            # Scraping company_type, employee_count, ownership_status, company_age and headquarters                  \n",
    "            combined.append(i.find('span', class_='companyCardWrapper__interLinking').text.strip())\n",
    "\n",
    "\n",
    "    company_type_pattern = r'^([^\\d]*)\\|'  \n",
    "    #company_type_pattern = r'^((?!Public|Forbes Global 2000|Indian Unicorn|Conglomerate|Fortune India 500|Startup|Central|State|MNC)[^\\d]*)\\|'\n",
    "    employees_count_pattern = r'(\\d+.*? Employees)'\n",
    "    ownership_pattern = r\" \\| (Public|Forbes Global 2000|Indian Unicorn|Conglomerate|Fortune India 500|Startup|Central|State|MNC) \\|\"\n",
    "    age_pattern = r'(\\d+ years old)'\n",
    "    location_pattern = r\"\\d+ years old \\| (.*?) \\+\"\n",
    "\n",
    "    for entry in combined:\n",
    "        company_match = re.search(company_type_pattern, entry)\n",
    "        employees_match = re.search(employees_count_pattern, entry)\n",
    "        ownership_match = re.search(ownership_pattern, entry)\n",
    "        age_match = re.search(age_pattern, entry)\n",
    "        location_match = re.search(location_pattern, entry)\n",
    "        try:\n",
    "\n",
    "            if company_match:\n",
    "                company_type.append(company_match.group(1).strip())\n",
    "            else:\n",
    "                company_type.append('N/A')\n",
    "\n",
    "            if employees_match:\n",
    "                employee_count.append(employees_match.group(1).strip())\n",
    "            else:\n",
    "                employee_count.append('N/A')\n",
    "\n",
    "            if ownership_match:\n",
    "                ownership_status.append(ownership_match.group(1).strip())\n",
    "            else:\n",
    "                ownership_status.append('N/A')\n",
    "\n",
    "            if age_match:\n",
    "                company_age.append(age_match.group(1).strip())\n",
    "            else:\n",
    "                company_age.append('N/A')\n",
    "\n",
    "            if location_match:\n",
    "                headquarters.append(location_match.group(1).strip())\n",
    "            else:\n",
    "                headquarters.append('N/A')\n",
    "\n",
    "        except AttributeError:\n",
    "             # Handle cases where there's an AttributeError\n",
    "            company_type.append('N/A')\n",
    "            employee_count.append('N/A')\n",
    "            ownership_status.append('N/A')\n",
    "            company_age.append('N/A')\n",
    "            headquarters.append('N/A')   \n",
    "        \n",
    "    \n",
    "    df=pd.DataFrame({'name':name,\n",
    "    'rating':ratings,\n",
    "    'company_type': company_type,\n",
    "    'employee_count' : employee_count,\n",
    "    'ownership_status': ownership_status,\n",
    "    'company_age': company_age,\n",
    "    'head_quarters':headquarters,\n",
    "    'reviews' : reviews,\n",
    "    'salaries': salaries,\n",
    "    'interviews':interviews,\n",
    "    'jobs': jobs,\n",
    "    'benefits': benefits,\n",
    "                })\n",
    "    \n",
    "    final = final.append(df, ignore_index = True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45097fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final.head(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7935c",
   "metadata": {},
   "source": [
    "#### Put the data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc74e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('Assignment1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bb364",
   "metadata": {},
   "source": [
    "#### Description of attributes in the dataset.\n",
    "\n",
    "1. Name: Name of the company.\n",
    "2. rating: Rating of the company.\n",
    "3. company_type: The type of service provided by the company.\n",
    "4. Employee_count: Number of employees in the company.\n",
    "5. ownership_status: Ownership_status of the company which would be private, public.\n",
    "6. company_age: Age of the company.\n",
    "7. head_quarters: Location of company headquarter.\n",
    "8. reviews: The number of reviews about the company.\n",
    "9. Salaries: The number of entries talking about the salary the company offers.\n",
    "10. Interviews: The number of interviews conducted by the company.\n",
    "11. Jobs: The number of job openings or positions available in the company.\n",
    "12. Benefits: Number of entries talking about the benefits the company offers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e1bb6",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf60db2",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning and Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893db5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604f53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examine the tail of the dataset to find missing values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the overall dataset to find missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of missing values in the dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d222c",
   "metadata": {},
   "source": [
    "The dataset indeed has a lot of missing values. The missing values have to be treated prior to analysis and Data Modelling. So let's examine each column/attribute of the dataset for finding and treating missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdefa8",
   "metadata": {},
   "source": [
    "#### i. Examine the attribute \"company_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5742e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['company_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db186c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all the NAN values with \"other\"\n",
    "df['company_type'].fillna('Other', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ed83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company_type'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68e547",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "The attribute 'company_type' has 90 unique values and 'other' is also one of the unique category present in this column. So, instead of removing all 445 rows we decided to replace all the 'NAN' values with \"Other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78364b80",
   "metadata": {},
   "source": [
    "#### ii. Examine the attribute \"Ownership_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1958bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ownership_status'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961db46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of missing values\n",
    "(df['ownership_status'].isna().sum()/len(df))*100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the feature\n",
    "df.drop(columns = ['ownership_status'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec159207",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "More than 70% of values have NAN values hence we decide to drop this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532aedda",
   "metadata": {},
   "source": [
    "#### iii. Examine the attribute \"employee_count\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine employee_count column\n",
    "df['employee_count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495392f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employee_count'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = ['employee_count'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d37bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employee_count'] = df['employee_count'].str.replace('Employees', '')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc634a4",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "A total of 254 rows have missing values in them hence we decide to drop these rows and transform the data to make it suitable for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6238de",
   "metadata": {},
   "source": [
    "#### iv. Examine the attribute  \"company_age\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of missing values\n",
    "df['company_age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f31499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the missing values\n",
    "df.dropna(subset = ['company_age'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'years old' from the data\n",
    "df['company_age'] = df['company_age'].str.replace('years old', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d294a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the type to 'int'\n",
    "df['company_age'] = df['company_age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376712af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5caa621",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "A total of 532 rows have missing values hence we decide to drop these rows and transform the data to make it suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7ee45",
   "metadata": {},
   "source": [
    "#### v. Examine the column \"head_quarters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24706252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the nan values have been filled with 'Unknown'\n",
    "df['head_quarters'].fillna('Unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['head_quarters'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a447cd",
   "metadata": {},
   "source": [
    "##### Observations: \n",
    "Instead of removing the missing values we simply filled the missing values with 'unknown'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed17117",
   "metadata": {},
   "source": [
    "#### vi. Examine the columns \"reviews\", \"salaries\", \"interviews\", \"jobs\" and \"benefits\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e11290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019849e6",
   "metadata": {},
   "source": [
    "It can be observed that few of the values in these columns have '--' values, it represents that the data for these records is missing. Instead of dropping these rows, we decide to replace '--' values with '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d085022",
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews_missing = (df['interviews'] == '--').sum()\n",
    "jobs_missing = (df['jobs'] == '--').sum()\n",
    "benefits_missing = (df['benefits'] == '--').sum()\n",
    "print(\"missing values in the column interviews:\" , interviews_missing)\n",
    "print(\"missing values in the column jobs:\", jobs_missing)\n",
    "print(\"missing values in the column benefits:\", benefits_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62094e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['interviews','jobs','benefits']] = df[['interviews','jobs','benefits']].replace({'--' : '0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315de1b3",
   "metadata": {},
   "source": [
    "It can be observed that the columns 'reviews', 'salaries', 'interviews', 'jobs' and 'benefits' are in incompatible format. The values in these columns are represented in form of decimal points followed by 'k', example: 734.8k. So, we decided to convert the data to numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the string 'k' and mutipliying the number by 100 and converting the attribute to int type.\n",
    "df[['reviews','salaries','interviews','jobs','benefits']] = df[['reviews','salaries','interviews','jobs','benefits']].applymap(lambda x: float(str(x).replace('k', '')) * 1000 if 'k' in str(x) else float(x))\n",
    "df[['reviews','salaries','interviews','jobs','benefits']] = df[['reviews','salaries','interviews','jobs','benefits']].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d409ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8181200",
   "metadata": {},
   "source": [
    "No more missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f3846",
   "metadata": {},
   "source": [
    "####  vii. Examine duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated(keep=False)]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98632b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate values\n",
    "df = df.drop_duplicates()\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca37038",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "There are 565 duplicate rows in the dataset. Hence we remove the duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14164fac",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537e360",
   "metadata": {},
   "source": [
    "#### 3. Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d7b31",
   "metadata": {},
   "source": [
    "#### 3(i) Univariate analysis\n",
    "This kind of analysis involves visualizing each attribute for its distribution. The attributes used for this analysis are 'company_age','reviews', 'rating', 'salaries', 'interviews', 'jobs' and 'benefits'. Histogram has plotted for all the features in order to understand the distribution and skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48348de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_histogram(df, column):\n",
    "    fig = plt.figure(figsize = (6,5))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(df[column], bins = 20)\n",
    "\n",
    "    # Set title and labels\n",
    "    fig.suptitle(f\"Histogram for {column}\", fontweight =\"bold\", fontsize = 10, y = 0.92)\n",
    "    plt.xlabel(column.capitalize())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "    # Calculate and display skewness\n",
    "    skewness = df[column].skew()\n",
    "    plt.figtext(0.40, 0.8, f'skewness: {skewness:.2f}', fontsize=12)\n",
    "\n",
    "# Call the function for each column\n",
    "for column in ['company_age','reviews', 'rating', 'salaries', 'interviews', 'jobs', 'benefits']:\n",
    "    plot_histogram(df, column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f5d0",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "Histograms have been plotted for all numeric columns of our dataset and based on the plots certain observations can be made:\n",
    "1. 'company_age','reviews','salaries','interviews','jobs' and 'benefits columns are highly right skewed.\n",
    "2. The column 'ratings' is slighly left skewed with skewness -0.64.\n",
    "\n",
    "These attributes have undergo some sort of a transformation prior to our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d49597",
   "metadata": {},
   "source": [
    "#### 3(ii) Bivariate Analysis\n",
    "\n",
    "This kind of analysis involves analyzing two features at a time. Scatter plot has been plotted between numeric features to represent the relationship between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08333320",
   "metadata": {},
   "source": [
    "#### a) Ratings vs Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268494e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['rating'],df['reviews'])\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Reviews')\n",
    "plt.title('Scatter Plot for Ratings vs Reviews' ,fontweight =\"bold\", fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8da171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['reviews'] >= 65000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03a408e",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "It can be observed from the above scatter plot that the count of reviews is highest for the ratings ranging from 3.5 to 4.5. There is a data point for which has more than 65000 reviews. Upon investigation it was that the company \"TCS\" that had the highest number of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592e63d",
   "metadata": {},
   "source": [
    "#### b) Salaries vs Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ae762",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['salaries'],df['reviews'])\n",
    "plt.xlabel('salaries')\n",
    "plt.ylabel('Reviews')\n",
    "plt.title('Scatter Plot for Salaries vs Reviews',fontweight =\"bold\", fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71869497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['salaries'] >= 40000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345922b",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "From the plot it can be said that there is a positive linear relationship between \"salaries\" and 'reviews'. As the number of entries for salaries increase the count of reviews also increases. It be concluded that employees working in the top companies such as TCS, Accenture, Wipro and Infosys (to name a few) have listed out their salaries and hence have given the  most number of reviews.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd7889",
   "metadata": {},
   "source": [
    "#### c) Jobs vs Reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f878bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['jobs'],df['reviews'])\n",
    "plt.xlabel('jobs')\n",
    "plt.ylabel('Reviews')\n",
    "plt.title('Scatter Plot for Jobs vs Reviews',fontweight =\"bold\", fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feaa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['jobs'] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04763746",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df['jobs'] <= 500) & (df['reviews'] > 50000)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a561a8",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "The companies which higher number of job postings are Accenture followed by IBM, Randstad and Diverse Lynx. Similarly it can be observed that there is a company named 'TCS' which has lowest job postings but it has the highest reviews. Overall this plot also represents positive linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4e174",
   "metadata": {},
   "source": [
    "#### d) company_age vs Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['company_age'],df['reviews'])\n",
    "plt.xlabel('company age')\n",
    "plt.ylabel('Reviews')\n",
    "plt.title('Scatter Plot for company_age vs Reviews',fontweight =\"bold\", fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0140258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['company_age'] >= 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e80c18",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "There seems to be an outlier in this plot where a comapny has more than 2000 years of age. Upon investigation there are 8 such companies with wrong company_age. These fields have the year as value (2023) instead of age. so, hence we have decided to correct the age for these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4fe347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the value '2023' with '1'\n",
    "df.loc[df['company_age'] == 2023, 'company_age'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab25ed9",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7df818",
   "metadata": {},
   "source": [
    "#### 4(i):  Pandas Profiling on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e86d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "df_1 = pd.read_csv(\"C:\\\\Users\\\\Faheem\\\\Downloads\\\\Assignment1.csv\")\n",
    "profile = ProfileReport(df_1, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c40a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45697862",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"PandasProfilingOnRawData.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc50da",
   "metadata": {},
   "source": [
    "#### 4(ii) : Pandas profiling on Preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed83f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile2 = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile2.to_file(\"PandasProfilingOnCleanedData.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73059324",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa4b0e",
   "metadata": {},
   "source": [
    "#### 5. Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedfe97",
   "metadata": {},
   "source": [
    "For this step we encode one feature from our dataset: \"employee_count\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64308b1b",
   "metadata": {},
   "source": [
    "#### 5(i) Perform Ordinal Encoding on \"employee_count\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employee_count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6070bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'1 Lakh+ ' : 0,\n",
    "          '50k-1 Lakh ' : 1,\n",
    "          '10k-50k ' : 2,\n",
    "          '5k-10k ': 3,\n",
    "          '1k-5k ': 4,\n",
    "          '501-1k ' : 5,\n",
    "          '201-500 ': 6,\n",
    "          '51-200 ': 7,\n",
    "          '11-50 ': 8,\n",
    "          '1-10 ': 9        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e63bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['employee_count'].replace(mapper, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d960cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot( x = 'employee_count', data = df)\n",
    "plt.xticks(ticks=df['employee_count'].unique(), labels=['1 Lakh+', '50k-1 Lakh', '10k-50k', '5k-10k', '1k-5k', '501-1k', '201-500', '51-200', '11-50', '1-10'], rotation=45, ha='right')\n",
    "plt.title(\"Frequency distribution of employee_count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255aa8f1",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "'employee_count' column has been encoded to numeric values ranging from 0-9 and a count plot has been plotted and it can be observed from the plot that majority of the companies in the dataset have employee count between \"1k- 5k\" followed by \"501-1k\" and '51-200'. This indicates that most of the companies in our dataset are relatively smaller companies. On the other hand there are very few companies with employee count greater than 1 lakh.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23447241",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8775409",
   "metadata": {},
   "source": [
    "#### 6. Outlier detection\n",
    "\n",
    "Two approches have been used to find out outliers in the data.\n",
    "1. Inter quartile range method\n",
    "2. Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010efa64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['rating','reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "summary_statistics = df[columns].describe()\n",
    "summary_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73160e9c",
   "metadata": {},
   "source": [
    "#### 6(i) Interquartile Range Method:\n",
    "In this approach all the numeric columns such as 'rating', 'reviews', 'salaries', 'interviews', 'jobs', 'benefits' and 'company_age' have been used to extract the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cf3be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_outliers(df, columns):\n",
    "    outlier_info = {}\n",
    "\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        outliers = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "        outlier_count = outliers.sum()\n",
    "\n",
    "        outlier_info[column] = {\n",
    "            'outlier_count': outlier_count,\n",
    "            'IQR': IQR,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound\n",
    "        }\n",
    "\n",
    "    return outlier_info\n",
    "\n",
    "outlier_columns = ['rating', 'reviews', 'salaries', 'interviews', 'jobs', 'benefits','company_age']\n",
    "outlier_info = find_outliers(df, outlier_columns)\n",
    "\n",
    "for column, info in outlier_info.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    print(f\"Count of outliers: {info['outlier_count']}\")\n",
    "    print(f\"IQR: {info['IQR']}\")\n",
    "    print(f\"Lower Bound: {info['lower_bound']}\")\n",
    "    print(f\"Upper Bound: {info['upper_bound']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feddfcb7",
   "metadata": {},
   "source": [
    "#### 6(ii) Box plot method\n",
    "All the outliers in the every respective numeric columns have been plotted with the help of box plot using the ploty library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa364a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns for which you want to create box plots\n",
    "columns = ['rating','reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "\n",
    "# Loop through the columns and create a box plot for each one\n",
    "for column in columns:\n",
    "    fig = px.box(df, y=column)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title='Box plot of ' + column,\n",
    "        xaxis_title= column,\n",
    "        yaxis_title='Frequency'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32e961",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "Both the approches show us that the data indeed has many outliers.\n",
    "The value which resides below the lower bound and the values that resides above the upper bound are considered to be outliers.\n",
    "Based on the calculations made by the Inter quartile range method and box plot method the number of outliers in each column is:\n",
    "1. Ratings: 131\n",
    "2. Reviews: 1049\n",
    "3. Salaries: 1038\n",
    "4. Interviews: 1053\n",
    "5. jobs: 1054\n",
    "6. Benefits : 1020\n",
    "7. company_age : 648\n",
    "\n",
    "\n",
    "The data is heavily skewed as a result the whiskers look very small and almost invisible. The outliers present in the data needs to be addressed using suitable techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ada60",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766c5b4",
   "metadata": {},
   "source": [
    "#### 7. Handling Outliers:\n",
    "1. Quantile-based Flooring and Capping\n",
    "2. Trimming\n",
    "3. Log Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d4f43",
   "metadata": {},
   "source": [
    "#### i. Quantile based flooring and capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ea2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataset and perform transformations on the copied version of the dataset.\n",
    "df2 = df.copy()\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns that need to be transformed\n",
    "columns = ['rating','reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "\n",
    "def floorCap(df2, columns):\n",
    "    #loop throught the columns \n",
    "    for column in columns:\n",
    "        #set a floor and cap value\n",
    "        floor = df2[column].quantile(0.10)\n",
    "        cap = df2[column].quantile(0.85)\n",
    "        #values which are less than floor and cap value are replaced with the floor and cap values.\n",
    "        df2[column] = np.where(df2[column]< floor, floor, df2[column])\n",
    "        df2[column] = np.where(df2[column]> cap, cap, df2[column])\n",
    "        \n",
    "floorCap(df2, columns)\n",
    "print(\"shape of the dataset after flooring and capping :\", df2.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70bd56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to create box plots\n",
    "columns = ['rating','reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "\n",
    "# Loop through the columns and create a box plot for each one\n",
    "for column in columns:\n",
    "    fig = px.box(df2, y=column)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title='Box plot of ' + column,\n",
    "        xaxis_title= column,\n",
    "        yaxis_title='Frequency'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df2, column):\n",
    "    fig = plt.figure(figsize = (6,5))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(df2[column], bins = 20)\n",
    "\n",
    "    # Set title and labels\n",
    "    fig.suptitle(f\"Histogram for {column}\", fontweight =\"bold\", fontsize = 10, y = 0.92)\n",
    "    plt.xlabel(column.capitalize())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "    # Calculate and display skewness\n",
    "    skewness = df2[column].skew()\n",
    "    plt.figtext(0.65, 0.8, f'skewness: {skewness:.2f}', fontsize=12)\n",
    "\n",
    "# Call the function for each column\n",
    "for column in ['company_age','reviews', 'salaries', 'interviews', 'jobs', 'benefits']:\n",
    "    plot_histogram(df2, column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a44a33",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "1. In this approach all the values which are below and above the lower bound and upper bound respectively are replaced with a \"floor and a cap\" value. After a lot of trial and error the optimal floor value was found to be be 0.10 quantile, and the cap value was found to be 0.85 quantile.\n",
    "2. Box plots for each numeric attributes have been plotted and it can be observed that all the outliers have been significantly replaced with the floor and cap value. The whsikers are also clearly presented unlike for the other plot where the outliers were not handled and as a result the whiskers were compressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54032c",
   "metadata": {},
   "source": [
    "#### ii. Trimming:\n",
    "In this approch we remove all the values which are less than the lower bound and remove all the values greater than the upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataset and perform transformations on the copied version of the dataset.\n",
    "df3 = df.copy()\n",
    "\n",
    "def trim(df3, columns):\n",
    "# Loop through the columns\n",
    "    for column in columns:\n",
    "        Q1 = df3[column].quantile(0.25)\n",
    "        Q3 = df3[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        index = df3[(df3[column] < lower_bound) | (df3[column] > upper_bound)].index\n",
    "        # drop the values which are less than the lower bound and values which are greater than the upper bound.\n",
    "        df3.drop(index, inplace = True) \n",
    "\n",
    "# columns that need to be transformed\n",
    "columns = ['rating','reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "trim(df3, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of dataset after trimming: \",df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af3f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to create box plots\n",
    "columns = ['rating','reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "\n",
    "# Loop through the columns and create a box plot for each one\n",
    "for column in columns:\n",
    "    fig = px.box(df3, y=column)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title='Box plot of ' + column,\n",
    "        xaxis_title= column,\n",
    "        yaxis_title='Frequency'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350d8de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_histogram(df3, column):\n",
    "    fig = plt.figure(figsize = (6,5))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(df3[column], bins = 20)\n",
    "\n",
    "    # Set title and labels\n",
    "    fig.suptitle(f\"Histogram for {column}\", fontweight =\"bold\", fontsize = 10, y = 0.92)\n",
    "    plt.xlabel(column.capitalize())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "    # Calculate and display skewness\n",
    "    skewness = df3[column].skew()\n",
    "    plt.figtext(0.65, 0.8, f'skewness: {skewness:.2f}', fontsize=12)\n",
    "\n",
    "# Call the function for each column\n",
    "for column in ['company_age','reviews', 'salaries', 'interviews', 'jobs', 'benefits']:\n",
    "    plot_histogram(df3, column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d73c7d",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "Trimming is the easiest and the most simplest way to remove outliers from the data but, the size of the dataset gets reduced as a result of trimming. Prior to trimming the dataset had 8500 instances after trimming the size of the dataset got reduced to 5145 instances. Even though this approach is easy we might ultimately loose important information as a result of trimming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7509a7f",
   "metadata": {},
   "source": [
    "#### iii. Log Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d810b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns which need to be transformed\n",
    "columns = ['reviews', 'salaries', 'interviews', 'jobs', 'benefits', 'company_age']\n",
    "\n",
    "def log_trans(df, columns):\n",
    "    #loop throught the function to access each column\n",
    "    for column in columns:\n",
    "        # use map and lambda to apply log function to the values\n",
    "        df[column] = df[column].map(lambda i: np.log(i) if i > 0 else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = log_trans(df, columns)\n",
    "print('shape of dataset: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe98858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to create box plots\n",
    "columns = ['reviews','salaries','interviews','jobs','benefits','company_age']\n",
    "\n",
    "# Loop through the columns and create a box plot for each one\n",
    "for column in columns:\n",
    "    fig = px.box(df, y=column)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        title='Box plot of ' + column,\n",
    "        xaxis_title= column,\n",
    "        yaxis_title='Frequency'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9413078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_histogram(df, column):\n",
    "    fig = plt.figure(figsize = (6,5))\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(df[column], bins = 20)\n",
    "\n",
    "    # Set title and labels\n",
    "    fig.suptitle(f\"Histogram for {column}\", fontweight =\"bold\", fontsize = 10, y = 0.92)\n",
    "    plt.xlabel(column.capitalize())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "    # Calculate and display skewness\n",
    "    skewness = df[column].skew()\n",
    "    plt.figtext(0.65, 0.8, f'skewness: {skewness:.2f}', fontsize=12)\n",
    "\n",
    "# Call the function for each column\n",
    "for column in ['company_age','reviews', 'salaries', 'interviews', 'jobs', 'benefits']:\n",
    "    plot_histogram(df, column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388805a",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "1. Log Transformation has been applied to all the numeric columns except for 'Ratings'. 'Ratings' are normally distributed and hence they do not require any additional transformation.\n",
    "2. Prior to logarithmic transformation the data was heavily right skewed (for example: the attribute 'salaries' had a skewness score of \"30.4\" prior to transformation and the score obtained after log operation is \"0.80\"). The data looks fairly distributed after logarithmic transformation. I see that the data is slightly skewed. But overall, it was a drastic change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0ba99",
   "metadata": {},
   "source": [
    "#### Outcome of all the outlier treatment methods and which method is the best method!!??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3dfe48",
   "metadata": {},
   "source": [
    "The three methods discussed above have successfully removed outliers but the outcomes of the three approches were different.\n",
    "1. The first approach flooring and capping is a decent method where in all the value below and upper the lower bound are replaced with floor and cap value. Though the shape of the dataset remains same, there is a lot trail and error involved in order to find the best floor and cap value. And replacing the extreme values with predefined value might be not beneficial as it will lead to loss of information, especially if the outliers have genuine values.\n",
    "2. The trimming method is the most easiest method to handle outliers in which you tend to remove all the values which are below and above the upper bound. This method reduces the size of the dataset and hence there is a loss of information. This is not the most suitable approch when handling outliers because we might end up loosing valuable information through which insights could have been generated.\n",
    "3. Logarithmic transformation is the most suitable to handle outliers and skewed data. This methods involves applying log -base 10 to every data point as a result of this operation the extreme values are reduced, and the differences among smaller values become more apparent. It also removes the skewness present in the data and makes the data look normally distributed. This is the most widely used approach to treat outliers because we do not loose upon the important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdee4de",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9106478",
   "metadata": {},
   "source": [
    "#### 8. Cluster Analysis:\n",
    "\n",
    "In this step we cluster all the numeric attributes present in the dataset such as ['ratings', 'reviews','jobs','salaries','interviews','Benefits' and 'company_age'.]. Clustering these features can give insights about high performing vs low performing companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d861043",
   "metadata": {},
   "source": [
    "#### 8(i): K-Means Clustering:\n",
    "\n",
    "\n",
    "Things which need to be performed are:\n",
    "1. Distance based algorithms are sensitive to scales as a result, scaling the data becomes important in order to bring all the numeric features to the same scale.\n",
    "2. Since we are using 7 features for clustering, We perform PCA on the data to reduce the dimensions of the dataset to '2'. so that it becomes easy for us to visualize.\n",
    "3. Since k-means requires us to provide number of clusters before hand, We find optimal number of clusters using elbow method.\n",
    "4. And then we fit our scaled data using k-Means algorithm and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the numeric columns will be used for clustering\n",
    "columns = ['rating','reviews','jobs','salaries','company_age']\n",
    "# all the columns in the dataframe will be treated as integers.\n",
    "numeric_columns = df[columns].apply(pd.to_numeric, errors='coerce')\n",
    "#Scale the data prior to clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af16581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pca to narrow down the dimensions \n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the principal components\n",
    "principal_components = pca.components_\n",
    "print(\"Principal Components:\", principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the principal components into a data frame.\n",
    "components_df = pd.DataFrame(principal_components, columns=numeric_columns.columns)\n",
    "# Print the DataFrame\n",
    "print(\"Principal Components DataFrame:\")\n",
    "components_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dddec",
   "metadata": {},
   "source": [
    "##### Elbow method- to find optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714905d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inertia = []\n",
    "# Iterate over the range of clusters\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    # fit the model\n",
    "    kmeans.fit(X_scaled)\n",
    "    # find and append sum of squared differences between each datapoint and its associated cluster.\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e2fdb",
   "metadata": {},
   "source": [
    "Looking at the plot it can be observed that there is a steep decrease after the cluster and hence we conclude that the optimal number of clusters required are '2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit k-means clustering \n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# put the results of clustering into the dataframe\n",
    "df['cluster'] = kmeans.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the centroids\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "#plot the scatter plot\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['cluster'], palette='viridis', s=50)\n",
    "# plot the centroids\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.title('K- Means Cluster Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658dfd7",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "1. The K-Means algorithm has successfully clustered data into 2 clusters.\n",
    "2. Few of the data points for the cluster '0' are far from its respective centroid.\n",
    "3. It can inferred that the cluster 0 has higher values are associated with higher values of reviews, salaries, jobs and company_age and a lower value of ratings.\n",
    "4. And the the cluster 1 has higher values associated with lower values of reviews, salaries and company_age and a higher value of jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3de40",
   "metadata": {},
   "source": [
    "#### 8(ii). Agglomerative Clustering \n",
    "\n",
    "This is a hierarchial clustering method which starts with one data point and each stage it merges the closest pair of clusters. The process is repaeted till one single cluster is formed.\n",
    "\n",
    "Steps involved are:\n",
    "1. Scale the data prior to clustering.\n",
    "2. Plot a dendogram to look at how clusters are merged. We use linkage method in order to see how the distance between the clusters is calculated. And the parameter we pass in the linkage method is 'ward'. This parameter finds the sum of squared difference between each point in the cluster and the mean of that cluster and then it performs merge operation. \n",
    "3. The optimal \"k\" value can found by looking at the dendogram. The highest vertical line which does not intersect with any cluster is said to be the optimal \"k\" value.\n",
    "4. we then fit the model using the optimal \"k\" value and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal number of clusters by plotting a dendogram\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "dendrogram(linkage_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eade5",
   "metadata": {},
   "source": [
    "The data can be clustered into 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Agglomerative clustering and predicting results on scaled data\n",
    "agglomerative_clustering = AgglomerativeClustering(n_clusters=2)\n",
    "df['A_cluster'] = agglomerative_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize the clusters using a scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['A_cluster'], palette='viridis', s=50)\n",
    "plt.title('Agglomerative Hierarchical Clustering Result')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929e3cc",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "1. The data has been clustered successfully into 2 clusters.\n",
    "2. The clustering seems to be more precise as compared with K-means.\n",
    "3. As mentioned previously the plot shows that cluster 0 has higher values are associated with higher values of reviews, salaries, jobs and company_age and a lower value of ratings.\n",
    "4. And the the cluster 1 has higher values associated with lower values of reviews, salaries and company_age and a higher value of jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e81fe",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78094bc1",
   "metadata": {},
   "source": [
    "1. Keita, Z. (2023, January 19). An introduction to hierarchical clustering in Python. DataCamp. https://www.datacamp.com/tutorial/introduction-hierarchical-clustering-python\n",
    "\n",
    "2. Sharma, P. (2023, October 23). What is Hierarchical Clustering in Python? Analytics Vidhya.\n",
    "https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/\n",
    "\n",
    "3. Kaloyanova, E. (2021, July 29). How to combine PCA and K-Means clustering in Python? 365 Data Science.https://365datascience.com/tutorials/python-tutorials/pca-k-means/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae5fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
